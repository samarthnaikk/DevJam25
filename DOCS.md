# Data Splitting Strategies for LLM Inference

| **Type of Splitting**      | **How it Works**                                                                 | **Pros**                                                    | **Cons**                                                   | **Best For**                                       |
|-----------------------------|---------------------------------------------------------------------------------|------------------------------------------------------------|-----------------------------------------------------------|---------------------------------------------------|
| Fixed-size chunks           | Split input into equal-size blocks (paragraphs, lines, or token-based).        | Simple, easy to implement, easy to distribute.            | May break context; output coherence may suffer.          | Embeddings, batch classification tasks          |
| Token-based chunks          | Split strictly by token count to fit model context limits.                     | Ensures each chunk fits in model context.                 | Needs tokenization; may break semantic units.            | LLMs with strict context limits                 |
| Semantic / Content-based    | Split according to paragraphs, sections, or topics.                             | Preserves meaning and coherence; better quality output.   | Uneven chunk sizes â†’ load imbalance; more preprocessing. | Summarization, document QA                       |
| Overlapping chunks          | Add overlap between adjacent chunks to preserve context at boundaries.          | Reduces context loss; better for tasks needing continuity | Extra computation; may need deduplication.              | Long documents, QA, code generation             |
| Sliding window              | Move a fixed-size chunk across input with a stride smaller than chunk size.     | Fine-grained context preservation; flexible.              | Higher redundancy; more computation.                     | Long-form generation, sensitive context tasks   |
| Hierarchical splitting      | Split input into large chunks, then smaller sub-chunks within each large chunk.| Preserves structure and context; scalable.                | Complex orchestration; higher latency.                   | Multi-document summarization, book processing  |
| Task-specific splitting     | Split based on task needs (e.g., relevant paragraphs for QA, sections for summaries). | Optimized for task performance and accuracy.              | Requires additional preprocessing; more complex.         | QA, retrieval-augmented tasks, targeted summaries |
| Hybrid / Combined splitting | Combine multiple strategies (semantic + overlap, or task-specific + hierarchical). | Balanced between coherence, accuracy, and speed.          | Most complex to implement.                                | Large, complex documents, multi-step tasks     |

